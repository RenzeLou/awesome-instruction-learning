<!-- omit in toc -->
# Awesome Textual Instruction Learning Papers


[![Awesome](https://awesome.re/badge.svg)](https://github.com/RenzeLou/awesome-instruction-learning) ![](https://img.shields.io/github/last-commit/RenzeLou/awesome-instruction-learning?color=green) ![](https://img.shields.io/badge/PaperNumber-0-blue) ![](https://img.shields.io/badge/PRs-Welcome-red)

A curated list of awesome **Instruction Learning** papers ğŸ”¥ğŸ”¥ğŸ”¥. 

Currently maintained by <ins>[Renze Lou](https://renzelou.github.io/) @ PennState</ins> and <ins>[Kai Zhang](https://drogozhang.github.io/) @ OhioState</ins>. 


**<font color='red'>Work still in progress</font>**  ğŸš€, **we appreciate any suggestions and contributions** â¤ï¸.

---

<!-- What is instruction learning?
Why instruction learning?
-->

<!-- TODO

## Our scope:
We aim to stay up-to-date with the most innovative developments in the field and gain valuable insights into the future of instruction-learning technology.ğŸ‘€

## Citation:
-->

<!-- omit in toc -->
## How to contribute?

If you have any suggestions or find any missed papers, feel free to reach out or submit a [pull request](https://github.com/RenzeLou/awesome-instruction-learning/pulls):

1. Use following markdown format.

```markdown
1. **Paper Title.** *Author 1, Author 2, and Author 3.* <ins>Conference/Journal/Preprint</ins> Year. [[pdf](link)]; [[other resources](link)].
```
<!-- >1. **Paper Title.** *Author 1, Author 2, and Author 3.* Conference/Journal/Preprint Year. [[pdf](link)]. -->


2. If one preprint paper has multiple versions, please use **the earliest submitted year**.
   
3. Display the papers in a **year descending order**.

---

<!-- omit in toc -->
## ğŸ” Table of Contents 

- [1. ğŸ“ Surveys and Tutorials](#1--surveys-and-tutorials)
- [2. ğŸ—‚ï¸ Taxonomies](#2-ï¸-taxonomies)
  - [2.1 Entailment-oriented Instruction](#21-entailment-oriented-instruction)
  - [2.2 PLM-oriented Instruction](#22-plm-oriented-instruction)
  - [2.3 Human-oriented Instruction](#23-human-oriented-instruction)
- [3. ğŸ“Š Analyses](#3--analyses)
  - [3.1 Scale](#31-scale)
  - [3.2 Explanability](#32-explanability)
  - [3.3 Robustness](#33-robustness)
  - [3.4 Negation](#34-negation)
  - [3.5 Others](#35-others)
- [4. ğŸš€ Applications](#4--applications)
  - [4.1 Human-Computer Interaction](#41-human-computer-interaction)
  - [4.2 Data and Feature Augmentation](#42-data-and-feature-augmentation)
- [5. ğŸ“š Datasets](#5--datasets)
- [6. ğŸ—’ï¸ Other Papers](#6-ï¸-other-papers)

---

## 1. ğŸ“ Surveys and Tutorials

<!-- There are several awesome surveys and tutorials on textual instruction learning. -->
<!-- To our knowledge, our survey is the first one to provide a comprehensive and broader overview of the field of instruction learning. -->
<!-- Since each survey focuses on specific in-context instruction, we attach a label to each of them to distinguish these topics.
, including `prompt`, `demonstrations`, `reasoning`, and `overview` (which means a broader perspective). -->


We use the label `comprehensive` to denote the papers with a more comprehensive perspective. While some other papers are more specific to a certain in-context instruction, including `prompt`, `demonstrations`, and `reasoning`.

1. **Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning.** *Renze Lou, Kai Zhang, and Wenpeng Yin.* <ins>Preprint</ins> 2023. [[pdf]()]; [[paper list](https://github.com/RenzeLou/awesome-instruction-learning)]. `comprehensive`.
   
2. **Learning from Task Instructions.** *Wenpeng Yin, Qinyuan Ye, Pengfei Liu, Xiang Ren, Hinrich SchÃ¼tze.* <ins>Tutorial@EMNLP</ins> 2023. `comprehensive`.

3. **Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.** *Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.* <ins>ACM Computing Surveys</ins> 2023. [[pdf](https://dl.acm.org/doi/pdf/10.1145/3560815)]; [[website](http://pretrain.nlpedia.ai/)]. `prompt`.
   
4. **A Survey on In-context Learning**. *Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui*. <ins>Preprint</ins> 2022. [[pdf](https://arxiv.org/pdf/2301.00234.pdf)]. `demonstrations`.
   
5. **Towards Reasoning in Large Language Models: A Survey.** *Huang, Jie, and Kevin Chen-Chuan Chang.* <ins>Preprint</ins> 2022. [[pdf](https://arxiv.org/pdf/2212.10403.pdf)]. `reasoning`.

6. **Reasoning with Language Model Prompting: A Survey.** *Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen.* <ins>Preprint</ins> 2022. [[pdf](https://arxiv.org/pdf/2212.09597.pdf)]; `reasoning`.


## 2. ğŸ—‚ï¸ Taxonomies

In our paper, we divide the textual instructions into three taxonomies.

<!-- TODO: add pic. -->

### 2.1 Entailment-oriented Instruction

1. 1. **Paper Title.** *Author 1, Author 2, and Author 3.* <ins>Conference/Journal/Preprint</ins> Year. [[pdf](link)]; [[other resources](link)].
   
2. **Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach.** *Wenpeng Yin, Jamaal Hay, Dan Roth* *.* <ins>EMNLP</ins> 2019. [[pdf](https://arxiv.org/pdf/1909.00161.pdf)].


### 2.2 PLM-oriented Instruction


### 2.3 Human-oriented Instruction


## 3. ğŸ“Š Analyses

### 3.1 Scale

### 3.2 Explanability

### 3.3 Robustness

### 3.4 Negation

### 3.5 Others

## 4. ğŸš€ Applications

### 4.1 Human-Computer Interaction

### 4.2 Data and Feature Augmentation

## 5. ğŸ“š Datasets

## 6. ğŸ—’ï¸ Other Papers

---